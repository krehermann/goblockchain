# Refactor public key

We had been hacking around with our public key wire format. Since we are using gob encoding
we had to register our PublicKey. We achieved this with

 `init() {gob.Register}`

 However, this relies on the PublicKey struct exposing all the public fields to be serializable

 In go 1.19 this is not true. That is, we got lucky and were used unsupported functionality

 The original problem is really that we need wire-friendly data types in our struct so
 that gob.Encode just works.

 With this insight, a solution is to change our 
 ```
 type PublicKey struct {
    *ecdsa.PublicKey
 }
 ```
 to 
 ` type PublicKey []byte`

 where the value of the new kind of PublicKey is a byte slice generated by our underlying 
 crypto algorithm. Still `[]byte` is support supported by `gob.Encode` we are good to go
 on the wire format.

 Then the problem is to refactor the code to support this new representation. A clever trick
 is to make a parallel implemenation, like `PublicKeyRefactor` and parallel implementations
 of each func in our crypto lib that operate on or return a `PublicKey`

 eg
 ```
 func foo(k PublicKey){}
 func fooNew(k PublicKeyRefactor)
```

then modify the tests our lib to use `PublicKeyRefactor` instead of `PublicKey`. Once this 
passes, we can rename delete the first impl of `PublicKey` and rename `PublicKeyRefactor ->
PublicKey`. 

This trick isolated the refactor into only the crypto package -- everything that depends 
on the public funcs of PublicKey are blissfully unaware.

Any reader other than me might think this is all trivial -- but I have often skipped the 
parallel implemenation, and especially subbing our the tests, which lead to a lot of 




Implement TCP transport

The big learning lesson is that we fucked up the initial transport design.

We knew that we want to get to TCP, UDP eventually. But we didn't really think ahead to that very much. 

Simple things that would have helped alot:
Seperate the transport and connections -- our local transport should have between composed of local connections that implement
a subset of of the net.Conn interface

this would have made us use a standard interface for connections, which would be easier to implement for other kinds of connections.

basically we mixed up the transport and the connections. this pain was also felt in the local transport refactor that had to go into
supporting syncing blocks. in that case, we needs to make the local transport bi directional. ultimately we needed bi-directional connections
by intermingling the transport and connections, we made a lot of extra work for ourselves.

said differently, even though we didn't know exactly what we wanted to build a little more 'standing on the shoulders giants' by
adopting and conforming to standard interfaces and composition would have saved a ton to later work. it's a good lesson.




Refactoring transport in support of block syncing
Refactor transport connections


the binding between the transport and the server is broken.

initTransports is supposed to own consumption from peers.

however the transports connect on their own. they broadcast messages to each
other. since we are only using local transports, there is a path -- which is
the only path we've been using -- were the transports connect to each other,
no peers are configured to the server, and yet messages get from one server
to the other.

either transport bookkeeping is broken, or the server consumption.

one thought is to formalize joining a server. a.Join(b) this would be responsible for setting up a consumer in b of a and vice versa.


Another problem along the way is was that the implementation of LocalTransport.Connect was uni-directional. In retrospect, i  think
this caused a lot of confusion. It mixed up the notion of a server peer with a transport connection. That is, because the connection was uni-directional
one could treat the transport as a peer in the network than a server. Instead, the transport connection needs to be bi-directional, and the server layer 
needs to own which servers it communicates with as peers. The pre-requisite for peer communication is a connection, but it is not sufficient.


Add network tests

The code in main was too confusing. Turn it into a real test

Episode 19: syncing blocks
https://youtu.be/zZkLI3FlW6k



Episode 18: custom smart contracts
https://youtu.be/fom_Zx2gXOk

Create tx that houses VM byte code for addition and broadcast to all nodes

Simplify the stack. I'm confused about how a stack-based VM can reuse variables.

But perhaps i'm overcomplicating it. I had been thinking about

a = 2
b =3
a+b
b-a

but i'm ignoring the fact that a program like that has to be compiled to byte code

the stack doesn't actually need to be reused. instead the compiler would 
be responsible for push a and b onto the stack for the corresponding addition
and subtraction expressions.

Anyway, the vm.Exec needs to Pop rather than read values. This became obvious 
when I tried to create a transaction with the instructions for the expressions
'abc'=2+5

I had implemented InstructionAdd as Read rather than Pop, but then it's not
possible to have the 'abc' and '7' next to each other in the stack when calling 
InstructionStore.


Episode 17: virtual machine contract state
https://youtu.be/rLxm4F6V1K4

Add generic state tracker to vm package. Diverged from videos by
making the state track `any` rather `[]byte`. Made the interface between
the stack and state easy, but perhaps will bite me later and need to be 
refactored.

structured table-driven testing paid dividends. i never would have been able
to follow all the byte codes and logic in the video. instead, i got lucky a 
couple Episodes ago and elected to have table-driven test for vm.Run, which
made the implementation in this segment much easier to think about than the
hardcode details in the live stream 

Episode 13: broadcasting blocks

I wasted a lot of time because I didn't have a test for DefaultRPCDecodeFunc.

I implemented the block broadcasting, but forget a single line in that func and chased 
my tail for a while. Next time, add a unit test first

Then, when I figured it out, i attempted to add a unit test, but got very confused
with RPC vs Message vs Decoded Message.

Added some clarity by changing the transport interface and adding comments

Self Study: Update the mempool implementation

Anthony did this offline here
https://github.com/anthdm/projectx/commit/6b7b3c6b6b46877950cf15d518b2dea4bb4842ad#diff-65ddbfca57544ad3aaedea5dbb6e4911bbe40753fb680c41a527dccff59311eb

More or less follow along. However, observe that his SortedMap is not really sorted. it
is ordered by Add operation.

A cleaner implementation would be a generic sorted map. As it stands the List and map 
are entangled a bit too much

Note: my implementation caps the pending transactions as well as the transaction history


Episode 12: add transactions to block

for now add all transactions from mempool into block. later need to limit
the number based on some size or complexity

plumb logger to blockchain. using functional opts very nice, with slight startup overhead

it's getting confusing with the broadcasting and transactions. need a way to think about
and test this


Episode 11: creating blocks

problem in server.Start:
1. the ticker is only needed is the server is a validatore
2. the rpc receiver can block the the ticker. this is very bad b/c it means that
    the blocktime is not reliable

choosen solution:
refactor the start code to be composed of two go routines, one to handle incoming rpc and
one to handle validator

Implement createBlock in server
Add DataHash to blocks -- done in Sign, which is different and seemingly (much) easier than the validatore



